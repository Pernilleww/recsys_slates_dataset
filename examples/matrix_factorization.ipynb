{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Example: Matrix Factorization\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/finn-no/recsys-slates-dataset/blob/main/examples/matrix_factorization.ipynb)  \n",
    "This notebook gives an example implementation of a collaborative filtering matrix factorization model on the FINN Recsys Slates Dataset.\n",
    "It is compatible with google colab, and can be run interactive by using the \"Open in Colab\"-button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define parameters for this run in a dictionary\n",
    "param = {\n",
    "    'dim' : 9,\n",
    "    'batch_size' : int(1e5),\n",
    "    'effective_batch_size' : int(2e6),\n",
    "    'sample_uniform_slates' : True, # If true, \n",
    "    'num_epochs': 100,\n",
    "    'overfit_batches' : False,\n",
    "    'name' : 'MatrixFactorization-CategoricalLoss'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary hack to load lightning branch:\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/finn/tensorflow/personal-scratch/recsys_slates_dataset/examples/\")\n",
    "sys.path.append(\"../../recsys_slates_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 10:45:51,792 Load data..\n",
      "2021-08-11 10:45:51,793 Download data if not in data folder..\n",
      "2021-08-11 10:45:51,795 Downloading data.npz\n",
      "2021-08-11 10:45:51,795 Downloading ind2val.json\n",
      "2021-08-11 10:45:51,796 Downloading itemattr.npz\n",
      "2021-08-11 10:45:51,797 Done downloading all files.\n",
      "2021-08-11 10:45:51,798 Load data..\n",
      "2021-08-11 10:46:24,414 Loading dataset with slate size=torch.Size([2277645, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-11 10:46:25,089 Loading dataset with slate size=torch.Size([2277645, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-11 10:46:25,104 Loading dataset with slate size=torch.Size([113882, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-11 10:46:25,124 Loading dataset with slate size=torch.Size([113882, 20, 25]) and uniform candidate sampling=False\n",
      "2021-08-11 10:46:25,126 In train: num_users: 2277645, num_batches: 23\n",
      "2021-08-11 10:46:25,127 In valid: num_users: 113882, num_batches: 2\n",
      "2021-08-11 10:46:25,128 In test: num_users: 113882, num_batches: 2\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from recsys_slates_dataset import lightning_helper\n",
    "dm = lightning_helper.SlateDataModule(num_workers=12, **param)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model\n",
    "We implement a simple Matrix Factorization model using categorical losses (instead of the traditional Gaussian loss).\n",
    "Given a slate $S$ shown to the user $u$, the likelihood of clicking a specific item $c$ is:\n",
    "\n",
    "$$ \\frac{e^{z_u *v_c}}{\\sum_{i \\in S} e^{z_u *v_c}} $$ \n",
    "\n",
    "where \n",
    "$z_u$ is a parameter vector for user $u$,  \n",
    "$v_i$ is a parameter vector for item $i$,  \n",
    "and $x*y$ is the inner product between $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class SimilarityDot(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, Z, V):\n",
    "        return (Z * V).sum(-1)\n",
    "\n",
    "def dict_chunker(dict_of_seqs, size):\n",
    "    \"Iterates over the first dimension of a dict of sequences\"\n",
    "    length = len(dict_of_seqs[list(dict_of_seqs.keys())[0]]) # length of first idex\n",
    "    return ( {key : seq[pos:pos + size] for key, seq in dict_of_seqs.items()} for pos in range(0, length, size))\n",
    "\n",
    "class MatrixFactorization(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_users, \n",
    "        num_items,\n",
    "        dim=2, \n",
    "        lr_start=1e-3,\n",
    "        optim=\"adam\",\n",
    "        *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.score_func = SimilarityDot()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        torch.manual_seed(1)\n",
    "        self.itemvec = nn.Embedding(self.hparams.num_items, self.hparams.dim)\n",
    "        nn.init.uniform_(self.itemvec.weight, a=-0.05, b=0.05)\n",
    "        self.uservec = nn.Embedding(self.hparams.num_users, self.hparams.dim)\n",
    "        nn.init.uniform_(self.uservec.weight, a=-0.05, b=0.05)\n",
    "\n",
    "    def loglik(self, batch):\n",
    "        # Get user and item parameters:\n",
    "        # Dimensions of tensors: [user/batch, interaction/step, item/slate, dim]\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "        v_action = self.itemvec(batch['slate'])\n",
    "\n",
    "        # Compute the similarity (dot product) between the users and items for all items in all slates:\n",
    "        scores = self.score_func(zetas,v_action)\n",
    "\n",
    "        # Set effectively zero probability for special Ids (0 is pad and 2 is UNK).\n",
    "        # These scores are log, so -100 is effectively 0: exp(-100)=4e-44\n",
    "        scores[(batch['slate'] == 2) | (batch['slate'] == 0)] = -100\n",
    "\n",
    "        # Flatten all Tensors to [user, slatelength]\n",
    "        # This simplifies the computation of the loss\n",
    "\n",
    "        mask = (batch['phase_mask']*(batch['click']>2)).bool()\n",
    "        scores_flat = scores[mask]\n",
    "        click_idx_flat = batch['click_idx'][mask]\n",
    "\n",
    "        # Compute the log likelihood of the observations:\n",
    "        # We use a categorical loss\n",
    "        loglik = dist.Categorical(logits=scores_flat).log_prob(click_idx_flat).sum()\n",
    "        return loglik\n",
    "    \n",
    "    #  TRAINING FUNCTIONS\n",
    "    def step(self, batch, batch_idx, phase):\n",
    "        stats = {}\n",
    "\n",
    "        stats['loglik'] = self.loglik(batch)\n",
    "        \n",
    "        # Since we are doing stochastic gradient decsent, \n",
    "        # multiply with the data factor to get estimate of the loss for the whole dataset:\n",
    "        data_factor = (self.hparams.num_users / batch['click'].size(0))\n",
    "        stats['loss'] = -(stats['loglik']*data_factor)\n",
    "\n",
    "        # Report loss and loglik:\n",
    "        with torch.no_grad():\n",
    "            for key, val in stats.items():\n",
    "                self.log(f\"{phase}/{key}\", val, on_step=False, on_epoch=True, sync_dist= (phase!=\"train\"))\n",
    "        \n",
    "        return stats['loss']\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Report mean absolute values of parameters:\n",
    "        for key, par in self.named_parameters():\n",
    "            self.log(f\"param/{key}-L1\", par.data.abs().mean(), on_step=False, sync_dist=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"train\")\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"valid\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pars = self.parameters()\n",
    "        optimizer = torch.optim.AdamW(pars, lr=self.hparams.lr_start)\n",
    "        return optimizer\n",
    "\n",
    "    # PREDICT FUNCTIONS BELOW HERE\n",
    "    @torch.jit.export\n",
    "    def forward_items(\n",
    "        self, \n",
    "        batch : Dict[str, torch.Tensor], \n",
    "        targets: Optional[torch.Tensor]=None, \n",
    "        t_rec: int=-1):\n",
    "        \"\"\" \n",
    "        Given a batch of data, estimate scores for all items in target.\n",
    "        If target is None, use all items.\n",
    "        NB: This function is very memory intensive. Need small batch sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        if targets is None:\n",
    "            targets = torch.arange(self.hparams.num_items,device=self.device)\n",
    "\n",
    "        target_vecs = self.itemvec(targets).unsqueeze(-2)\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        scores = self.score_func(zetas,target_vecs).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recommend_batch(self, batch: Dict[str, torch.Tensor], num_rec=1, chunksize=3, t_rec=-1, **kwargs):\n",
    "        topk = torch.zeros((len(batch['click']), num_rec), device=self.device)\n",
    "\n",
    "        i = 0\n",
    "        for batch_chunk in dict_chunker(batch, chunksize):\n",
    "            pred = self.forward_items(batch=batch, t_rec=t_rec)\n",
    "            vals, topk_chunk = pred[:,3:].topk(num_rec, dim=1)\n",
    "            topk_chunk = 3+topk_chunk\n",
    "\n",
    "            topk[i:(i + len(pred))] = topk_chunk\n",
    "            i += len(pred)\n",
    "\n",
    "        return topk\n",
    "\n",
    "\n",
    "model = MatrixFactorization(num_items = dm.num_items, num_users = dm.num_users, **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add callbacks\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"valid/loglik\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "cb = [\n",
    "    checkpoint_callback,\n",
    "    # pl.callbacks.LearningRateMonitor(),\n",
    "    lightning_helper.CallbackPrintRecommendedCategory(dm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type          | Params\n",
      "---------------------------------------------\n",
      "0 | score_func | SimilarityDot | 0     \n",
      "1 | itemvec    | Embedding     | 11.8 M\n",
      "2 | uservec    | Embedding     | 20.5 M\n",
      "---------------------------------------------\n",
      "32.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.3 M    Total params\n",
      "129.222   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e66688429243259871053683b77f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c727a3f2d04157885feb1d07fdc21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    overfit_batches=param.get('overfit_batches', False), # for fast dry-runs\n",
    "    callbacks=cb,\n",
    "    logger = pl.loggers.TensorBoardLogger(f\"logs\", name=param['name']),\n",
    "    max_epochs=param['num_epochs'], \n",
    "    gpus= -1, \n",
    "    accumulate_grad_batches= int(param['effective_batch_size']/param['batch_size']), \n",
    "    weights_summary='full',\n",
    "    )\n",
    "\n",
    "#%% TRAIN\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pytorch': conda)",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
