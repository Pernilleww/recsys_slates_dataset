{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Example: Matrix Factorization\n",
    "\n",
    "This notebook gives an example implementation of a collaborative filtering matrix factorization model on the FINN Recsys Slates Dataset.\n",
    "It is compatible with google colab, and can be run interactive by using the \"Open in Colab\"-button.\n",
    "If using Google Colab it is highly recommended to use a runtime with GPU for speedup _and_ to ensure that there is sufficient memory. \n",
    "Go to \"Runtime\" > \"Change Runtime Type\" and choose GPU hardware accelerator.  \n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/finn-no/recsys-slates-dataset/blob/main/examples/matrix_factorization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab: Install additional dependencies\n",
    "!pip install pytorch-lightning recsys_slates_dataset -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/personal/recsys_slates_dataset/examples'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Define parameters for this run in a dictionary\n",
    "param = {\n",
    "    'dim' : 9,\n",
    "    'batch_size' : int(1e5),\n",
    "    'effective_batch_size' : int(2e6),\n",
    "    'sample_candidate_items' : 4, # If true, the dataloader adds an additional datapoint to each batch, \"allitem\", which is randomly sampled items to be used as negative feedback\n",
    "    'num_epochs': 100,\n",
    "    'overfit_batches' : False,\n",
    "    'name' : 'MatrixFactorization-CategoricalLoss'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-13 13:59:36,081 Load data..\n",
      "2021-08-13 13:59:36,082 Download data if not in data folder..\n",
      "2021-08-13 13:59:36,083 Downloading data.npz\n",
      "2021-08-13 13:59:36,083 Downloading ind2val.json\n",
      "2021-08-13 13:59:36,084 Downloading itemattr.npz\n",
      "2021-08-13 13:59:36,084 Done downloading all files.\n",
      "2021-08-13 13:59:36,084 Load data..\n",
      "2021-08-13 14:00:00,264 Loading dataset with slate size=torch.Size([2277645, 20, 25]) and number of negative samples=4\n",
      "2021-08-13 14:00:00,269 Loading dataset with slate size=torch.Size([113882, 20, 25]) and number of negative samples=4\n",
      "2021-08-13 14:00:00,276 Loading dataset with slate size=torch.Size([113882, 20, 25]) and number of negative samples=4\n",
      "2021-08-13 14:00:00,278 In train: num_users: 2277645, num_batches: 23\n",
      "2021-08-13 14:00:00,278 In valid: num_users: 113882, num_batches: 2\n",
      "2021-08-13 14:00:00,279 In test: num_users: 113882, num_batches: 2\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from recsys_slates_dataset import lightning_helper\n",
    "dm = lightning_helper.SlateDataModule(num_workers=0, **param)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Model\n",
    "We implement a simple Matrix Factorization model using categorical losses (instead of the traditional Gaussian loss).\n",
    "Given a slate $S$ shown to the user $u$, the likelihood of clicking a specific item $c$ is:\n",
    "\n",
    "$$ \\frac{e^{z_u *v_c}}{\\sum_{i \\in S} e^{z_u *v_c}} $$ \n",
    "\n",
    "where \n",
    "$z_u$ is a parameter vector for user $u$,  \n",
    "$v_i$ is a parameter vector for item $i$,  \n",
    "and $x*y$ is the inner product between $x$ and $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class SimilarityDot(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, Z, V):\n",
    "        return (Z * V).sum(-1)\n",
    "\n",
    "def dict_chunker(dict_of_seqs, size):\n",
    "    \"Iterates over the first dimension of a dict of sequences\"\n",
    "    length = len(dict_of_seqs[list(dict_of_seqs.keys())[0]]) # length of first idex\n",
    "    return ( {key : seq[pos:pos + size] for key, seq in dict_of_seqs.items()} for pos in range(0, length, size))\n",
    "\n",
    "class MatrixFactorization(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_users, \n",
    "        num_items,\n",
    "        dim=2, \n",
    "        lr_start=1e-3,\n",
    "        optim=\"adam\",\n",
    "        *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.score_func = SimilarityDot()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        torch.manual_seed(1)\n",
    "        self.itemvec = nn.Embedding(self.hparams.num_items, self.hparams.dim)\n",
    "        nn.init.uniform_(self.itemvec.weight, a=-0.05, b=0.05)\n",
    "        self.uservec = nn.Embedding(self.hparams.num_users, self.hparams.dim)\n",
    "        nn.init.uniform_(self.uservec.weight, a=-0.05, b=0.05)\n",
    "\n",
    "    def loglik(self, batch):\n",
    "        # Get user and item parameters:\n",
    "        # Dimensions of tensors: [user/batch, interaction/step, item/slate, dim]\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "        # Concatenate positive and negative items (first element is the positive one)\n",
    "        items = torch.cat((batch['click'].unsqueeze(-1), batch['allitem']), dim=-1)\n",
    "\n",
    "        # find the parameters vector corresponding to each item in the batch:\n",
    "        itemvecs_batch = self.itemvec(items)\n",
    "\n",
    "        # Compute the similarity (dot product) between the users and items for all items in all slates:\n",
    "        scores = self.score_func(zetas, itemvecs_batch)\n",
    "\n",
    "        # Set effectively zero probability for special Ids (0 is pad and 2 is UNK).\n",
    "        # These scores are log, so -100 is effectively 0: exp(-100)=4e-44\n",
    "        #scores[(batch['slate'] == 2) | (batch['slate'] == 0)] = -100\n",
    "\n",
    "        # Flatten all Tensors to [user, slatelength] (This simplifies the computation of the loss)\n",
    "        # We flatten by using a masking tensor that also selects the relevant data.\n",
    "\n",
    "        # Mask out data that are in a different phase AND datapoints that did not result in any clicks:\n",
    "        mask = (batch['phase_mask']*(batch['click']>=3)).bool()\n",
    "\n",
    "        scores_flat = scores[mask]\n",
    "\n",
    "        # Compute the \"allitem\" log likelihood of the observations:\n",
    "        # We use a categorical loss where all our positive signals are in the first dimension:\n",
    "        click_idx_flat = torch.zeros((scores_flat.size(0)), device=self.device)\n",
    "        loglik = dist.Categorical(logits=scores_flat).log_prob(click_idx_flat).sum()\n",
    "        return loglik\n",
    "    \n",
    "    #  TRAINING FUNCTIONS\n",
    "    def step(self, batch, batch_idx, phase):\n",
    "        stats = {}\n",
    "\n",
    "        stats['loglik'] = self.loglik(batch)\n",
    "        \n",
    "        # Since we are doing stochastic gradient decsent, \n",
    "        # multiply with the data factor to get estimate of the loss for the whole dataset:\n",
    "        data_factor = (self.hparams.num_users / batch['click'].size(0))\n",
    "        stats['loss'] = -(stats['loglik']*data_factor)\n",
    "\n",
    "        # Report loss and loglik:\n",
    "        with torch.no_grad():\n",
    "            for key, val in stats.items():\n",
    "                self.log(f\"{phase}/{key}\", val, on_step=False, on_epoch=True, sync_dist= (phase!=\"train\"))\n",
    "        \n",
    "        return stats['loss']\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Report mean absolute values of parameters:\n",
    "        for key, par in self.named_parameters():\n",
    "            self.log(f\"param/{key}-L1\", par.data.abs().mean(), on_step=False, sync_dist=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"train\")\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, batch_idx, phase=\"valid\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        pars = self.parameters()\n",
    "        optimizer = torch.optim.AdamW(pars, lr=self.hparams.lr_start)\n",
    "        return optimizer\n",
    "\n",
    "    # PREDICT FUNCTIONS BELOW HERE\n",
    "    @torch.jit.export\n",
    "    def forward_items(\n",
    "        self, \n",
    "        batch : Dict[str, torch.Tensor], \n",
    "        targets: Optional[torch.Tensor]=None, \n",
    "        t_rec: int=-1):\n",
    "        \"\"\" \n",
    "        Given a batch of data, estimate scores for all items in target.\n",
    "        If target is None, use all items.\n",
    "        NB: This function is very memory intensive. Need small batch sizes.\n",
    "        \"\"\"\n",
    "\n",
    "        if targets is None:\n",
    "            targets = torch.arange(self.hparams.num_items,device=self.device)\n",
    "\n",
    "        target_vecs = self.itemvec(targets).unsqueeze(-2)\n",
    "        zetas = self.uservec(batch['userId']).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        scores = self.score_func(zetas,target_vecs).squeeze(-1)\n",
    "        return scores\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recommend_batch(self, batch: Dict[str, torch.Tensor], num_rec=1, chunksize=3, t_rec=-1, **kwargs):\n",
    "        topk = torch.zeros((len(batch['click']), num_rec), device=self.device)\n",
    "\n",
    "        i = 0\n",
    "        for batch_chunk in dict_chunker(batch, chunksize):\n",
    "            pred = self.forward_items(batch=batch_chunk, t_rec=t_rec)\n",
    "            vals, topk_chunk = pred[:,3:].topk(num_rec, dim=1)\n",
    "            topk_chunk = 3+topk_chunk\n",
    "\n",
    "            topk[i:(i + len(pred))] = topk_chunk\n",
    "            i += len(pred)\n",
    "\n",
    "        return topk\n",
    "\n",
    "model = MatrixFactorization(num_items = dm.num_items, num_users = dm.num_users, **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Add callbacks\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"valid/loglik\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "cb = [\n",
    "    checkpoint_callback,\n",
    "    lightning_helper.Hitrate(dm, report_interval=100, num_rec=20),\n",
    "    lightning_helper.CallbackPrintRecommendedCategory(dm)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "2021-08-13 14:19:59,142 Downloading data.npz\n",
      "2021-08-13 14:19:59,142 Downloading ind2val.json\n",
      "2021-08-13 14:19:59,143 Downloading itemattr.npz\n",
      "2021-08-13 14:19:59,143 Done downloading all files.\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type          | Params\n",
      "---------------------------------------------\n",
      "0 | score_func | SimilarityDot | 0     \n",
      "1 | itemvec    | Embedding     | 11.8 M\n",
      "2 | uservec    | Embedding     | 20.5 M\n",
      "---------------------------------------------\n",
      "32.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.3 M    Total params\n",
      "129.222   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e667585b00b40e681bd82294e9abf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (23) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769baef15be84289ac537076255fe06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdf199ba1a04e33936bdd41245fc5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e7854bc7e04241b16911ba7b4a2a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21372f8b1ee24f3f93076c5c08b7c957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660556935a184309a6a3c67f33ddef37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbd16cc5dd24d13890a2d15668333b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d29b68e72da4170ad7a8ede4d33f930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0920e0f1bcc4cf39afa395a52a2d1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334d6d58e3494fbdac767436b15363a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1047: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    overfit_batches=param.get('overfit_batches', False), # for fast dry-runs\n",
    "    callbacks=cb,\n",
    "    logger = pl.loggers.TensorBoardLogger(f\"logs\", name=param['name']),\n",
    "    max_epochs=param['num_epochs'], \n",
    "    gpus= -1 if torch.cuda.is_available() else 0, \n",
    "    accumulate_grad_batches= int(param['effective_batch_size']/param['batch_size']), \n",
    "    weights_summary='full',\n",
    "    )\n",
    "\n",
    "#%% TRAIN\n",
    "trainer.fit(model, dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pytorch': conda)",
   "name": "python3812jvsc74a57bd03c5be2b06ed6dbdf00174833742462bf4dfeb93002accc42ac9edec65c60a8dd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
